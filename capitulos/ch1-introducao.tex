% ==============================================================================
% Capítulo 1 - Introdução
% ==============================================================================
\chapter{Introdução}
\label{sec-intro}

Equações diferencias são equações que descrevem uma relação entre uma função
e suas derivadas, são de particular interesse para as ciências naturais
por sua aplicação na modelagem de fenômenos naturais e de leis físicas.
A principal distinção entre elas é feita pelo número de variáveis independentes,
no caso de apenas uma variável, chama-se de equação parcial ordinária (EDO),
e equação diferencial parcial (EDP) para o caso mais de uma variável independente.
Muitas das equações de interesse para áreas como engenharia, física, 
ecologia, química e epidemiológia, apenas para nomear algumas áreas, 
não possuem soluções analíticas conhecidas, fazendo com a busca por métodos 
númericos seja uma área de pesquisa ativa da matemática aplicada. 
Ao longo dos século passado, foram desenvolvidos métodos para a solução de 
equações, como o método de diferenças finitas (MDF) e o 
método de elementos finitos (MEF).
Entretanto é necessário uma discretização do domínio das equações, gerando uma 
malha de pontos em que a solução será resolvida.
A criação desta malha (\textit{mesh}) não é uma tarefa trivial, e a qualidade 
da solução obtida está diretamente ligada a obtenção desta malha \cite{raissi-etal:19}.
Logo, há um interesse em métodos que não necessitem de malhas. 

Uma redes neural artificial, ou apenas rede neural, é um modelo de computação 
inspirado no funcionamento do cérebro, seu desenvolvimento remonta a trabalhos
pioneiros ainda nos anos 40 como \cite{mcculloch-pitts:1943-perceptron}.
O pontencial que redes neurais têm como solucionadores númericos de equações
diferencias pode ser facilmente observado, pois redes neurais são aproximadores
universais, ou seja são capazes de aproximar qualquer função, inclusive uma 
que seja a solução de uma equação diferencial.
Esta propriedade é atestada pelo teorema da aproximação universal, 
demonstrado primeiramente para redes com
largura arbritária e função sigmóide por \cite{cybenko:89}, e para redes 
com no minímo uma camada escondida por \cite{hornik:89-aprox-universais}. 
A versão do teorema demonstrada nesses artigos, atesta que
que redes neurais com uma largura suficientemente grande são capazes de aproximar 
qualquer função. Anos depois, em \cite{grippen:03-aprox-universais-profundidade},
a mesma propriedade foi demonstrada para redes com profundidade arbritária 
e largura fixa.

O pontencial das redes neurais para a solução numérica de equações diferencias 
já havia percebido nos anos 90, em trabalhos como \cite{psichogios-etal:92} que
incorporou uma rede neural na modelagem de um bioreator para a estimativa de
parâmetros que são difíceis de de serem estimados apenas com princípios físicos 
e químicos. Um outro exemplo que pode ser citado, é \cite{lagaris-etal:98},


\cite{meade-fernandez:94} e , entretanto, 
foi apenas com \cite{raissi-etal:19} e a introdução do conceito das 
\textit{Physics Informed Neural Networks} 
(PINNs) que se renovou o interesse em aplicar redes neurais para a solução de 
problemas científicos.
PINNs podem ser compreendidas como uma técnica avançada de regularização que, ao 
incorporar as equações que compõem um modelo à função de perda da rede neural. A
grande diferença da abordagem das PINNs em relação a propostas anteriores, é tratar
não apenas as equações que compõem o modelo como residuais a serem minimizados, mas 
também incorporar as condições de fronteira e iniciais. 

Esta abordagem foi possível apenas com o desenvolvimento de técnicas de 
diferenciação automática, pode-se citar \cite{wengert:64-diferenciao-automatica} 
para \textit{forward accumulation}, e \cite{linnainmaa:76-diferenciao-automatica} 
para \textit{inverse accumulation}, como alguns dos primeiro trabalhos a propor 
esta técnica. Entretanto, foi sua incorporação à bibliotecas como 
PyTorch \cite{pytorch:19} e TensorFlow \cite{tensorflow:16}, facilitando em muito
o uso de funções de perda mais complexas, que o uso de redes neurais para a solução
de equações diferencias se tornou atrativa. 

PINNs foram aplicados para diversos problemas de engenharia, por exemplo,
escoamento de fluxos incompressíveis \cite{jin-et-al:21-navier-stokes}, 
problemas epidemiológicos como \cite{shaier-etal:22-dinns}, problemas de formação
de padrões em modelos de reação-difusão \cite{giampaolo-etal:22-gray-scott},
solução de problemas de física quãntica \cite{jin-etal:2022-schrondiger}.

No artigo original, são utilizados \textit{Multi-layer Perceptrons} (MLPs)
como arquitetura das redes, mas há propostas com utilizando outras arquiteturas.
Uma proposta utilizando redes neurais convolucionais pode ser encontrada em 
\cite{shi-etal:24-convnet}. Uma proposta utilizando PINNs combinado com 
métodos Bayesianos pode ser encontrada em \cite{yang:21-bpinns}, esta 
abordagem é particulamente interessante para problemas inversos, ao transformar
a estimativa dos parâmetros numa distribuição, no lugar de um valor fixo.

Modelos compartimentais baseados em equações diferencias foram primeiramente 
introduzidos por \cite{kermack-mcKendrick:1927}, ao proporem o modelo 
\textit{Susceptible-Infected-Removed} (SIR). Baseados neste trabalho seminal,
foram propostos outros modelos com mais mais compartimentos como o 
\textit{SEIRD} \cite{giles:77-sird} que inclui um compartimento para individuos
que fora expostos a doença, ainda não manifestaram sintomas. Outro exemplo é o
\textit{SIRV} \cite{schlickeiser-kroger:21-sirv} que inclui um compartimento
para vacinados.

Este trabalho está organizado da seguinte forma: no capítulo \ref{sec-literatura} 
são formalizados os conceitos de PINNs e modelos compartimentais; 
no capítulo \ref{sec-proposta} é detalhado o método proposto por este 
trabalho e são definidos os experimentos para averiguar a efetiviade
do método; no capítulo \ref{sec-avaliacao} são apresentados os resultados e 
é feita uma avaliação dos mesmos; por fim, no capítulo \ref{sec-conclusoes} são
apresentadas as conclusões.    